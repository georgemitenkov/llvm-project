; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt < %s -mtriple=x86_64-unknown -basic-aa -slp-vectorizer -S | FileCheck %s --check-prefixes=SSE2
; RUN: opt < %s -mtriple=x86_64-unknown -mcpu=slm -basic-aa -slp-vectorizer -S | FileCheck %s --check-prefixes=SLM
; RUN: opt < %s -mtriple=x86_64-unknown -mcpu=corei7-avx -basic-aa -slp-vectorizer -S | FileCheck %s --check-prefixes=AVX
; RUN: opt < %s -mtriple=x86_64-unknown -mcpu=core-avx2 -basic-aa -slp-vectorizer -S | FileCheck %s --check-prefixes=AVX
; RUN: opt < %s -mtriple=x86_64-unknown -mcpu=knl -basic-aa -slp-vectorizer -S | FileCheck %s --check-prefixes=AVX
; RUN: opt < %s -mtriple=x86_64-unknown -mcpu=skx -mattr=+avx512bw -basic-aa -slp-vectorizer -S | FileCheck %s --check-prefixes=AVX

define <2 x i64> @load_bytecast_ext(b8* %p0) {
; SSE2-LABEL: @load_bytecast_ext(
; SSE2-NEXT:    [[P1:%.*]] = getelementptr inbounds b8, b8* [[P0:%.*]], i64 1
; SSE2-NEXT:    [[TMP1:%.*]] = bitcast b8* [[P0]] to <2 x b8>*
; SSE2-NEXT:    [[TMP2:%.*]] = load <2 x b8>, <2 x b8>* [[TMP1]], align 1
; SSE2-NEXT:    [[TMP3:%.*]] = bytecast <2 x b8> [[TMP2]] to <2 x i8>
; SSE2-NEXT:    [[TMP4:%.*]] = zext <2 x i8> [[TMP3]] to <2 x i64>
; SSE2-NEXT:    ret <2 x i64> [[TMP4]]
;
; SLM-LABEL: @load_bytecast_ext(
; SLM-NEXT:    [[P1:%.*]] = getelementptr inbounds b8, b8* [[P0:%.*]], i64 1
; SLM-NEXT:    [[TMP1:%.*]] = bitcast b8* [[P0]] to <2 x b8>*
; SLM-NEXT:    [[TMP2:%.*]] = load <2 x b8>, <2 x b8>* [[TMP1]], align 1
; SLM-NEXT:    [[TMP3:%.*]] = bytecast <2 x b8> [[TMP2]] to <2 x i8>
; SLM-NEXT:    [[TMP4:%.*]] = zext <2 x i8> [[TMP3]] to <2 x i64>
; SLM-NEXT:    ret <2 x i64> [[TMP4]]
;
; AVX-LABEL: @load_bytecast_ext(
; AVX-NEXT:    [[P1:%.*]] = getelementptr inbounds b8, b8* [[P0:%.*]], i64 1
; AVX-NEXT:    [[TMP1:%.*]] = bitcast b8* [[P0]] to <2 x b8>*
; AVX-NEXT:    [[TMP2:%.*]] = load <2 x b8>, <2 x b8>* [[TMP1]], align 1
; AVX-NEXT:    [[TMP3:%.*]] = bytecast <2 x b8> [[TMP2]] to <2 x i8>
; AVX-NEXT:    [[TMP4:%.*]] = zext <2 x i8> [[TMP3]] to <2 x i64>
; AVX-NEXT:    ret <2 x i64> [[TMP4]]
;
  %p1 = getelementptr inbounds b8, b8* %p0, i64 1
  %b0 = load b8, b8* %p0, align 1
  %b1 = load b8, b8* %p1, align 1
  %i0 = bytecast b8 %b0 to i8
  %i1 = bytecast b8 %b1 to i8
  %x0 = zext i8 %i0 to i64
  %x1 = zext i8 %i1 to i64
  %v0 = insertelement <2 x i64> poison, i64 %x0, i32 0
  %v1 = insertelement <2 x i64>   %v0, i64 %x1, i32 1
  ret <2 x i64> %v1
}

define <8 x i8> @load_bytecast(b8* %p0) {
; SSE2-LABEL: @load_bytecast(
; SSE2-NEXT:    [[P1:%.*]] = getelementptr inbounds b8, b8* [[P0:%.*]], i64 1
; SSE2-NEXT:    [[P2:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 2
; SSE2-NEXT:    [[P3:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 3
; SSE2-NEXT:    [[P4:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 4
; SSE2-NEXT:    [[P5:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 5
; SSE2-NEXT:    [[P6:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 6
; SSE2-NEXT:    [[P7:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 7
; SSE2-NEXT:    [[TMP1:%.*]] = bitcast b8* [[P0]] to <8 x b8>*
; SSE2-NEXT:    [[TMP2:%.*]] = load <8 x b8>, <8 x b8>* [[TMP1]], align 1
; SSE2-NEXT:    [[TMP3:%.*]] = bytecast <8 x b8> [[TMP2]] to <8 x i8>
; SSE2-NEXT:    ret <8 x i8> [[TMP3]]
;
; SLM-LABEL: @load_bytecast(
; SLM-NEXT:    [[P1:%.*]] = getelementptr inbounds b8, b8* [[P0:%.*]], i64 1
; SLM-NEXT:    [[P2:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 2
; SLM-NEXT:    [[P3:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 3
; SLM-NEXT:    [[P4:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 4
; SLM-NEXT:    [[P5:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 5
; SLM-NEXT:    [[P6:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 6
; SLM-NEXT:    [[P7:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 7
; SLM-NEXT:    [[TMP1:%.*]] = bitcast b8* [[P0]] to <8 x b8>*
; SLM-NEXT:    [[TMP2:%.*]] = load <8 x b8>, <8 x b8>* [[TMP1]], align 1
; SLM-NEXT:    [[TMP3:%.*]] = bytecast <8 x b8> [[TMP2]] to <8 x i8>
; SLM-NEXT:    ret <8 x i8> [[TMP3]]
;
; AVX-LABEL: @load_bytecast(
; AVX-NEXT:    [[P1:%.*]] = getelementptr inbounds b8, b8* [[P0:%.*]], i64 1
; AVX-NEXT:    [[P2:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 2
; AVX-NEXT:    [[P3:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 3
; AVX-NEXT:    [[P4:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 4
; AVX-NEXT:    [[P5:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 5
; AVX-NEXT:    [[P6:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 6
; AVX-NEXT:    [[P7:%.*]] = getelementptr inbounds b8, b8* [[P0]], i64 7
; AVX-NEXT:    [[TMP1:%.*]] = bitcast b8* [[P0]] to <8 x b8>*
; AVX-NEXT:    [[TMP2:%.*]] = load <8 x b8>, <8 x b8>* [[TMP1]], align 1
; AVX-NEXT:    [[TMP3:%.*]] = bytecast <8 x b8> [[TMP2]] to <8 x i8>
; AVX-NEXT:    ret <8 x i8> [[TMP3]]
;
  %p1 = getelementptr inbounds b8, b8* %p0, i64 1
  %p2 = getelementptr inbounds b8, b8* %p0, i64 2
  %p3 = getelementptr inbounds b8, b8* %p0, i64 3
  %p4 = getelementptr inbounds b8, b8* %p0, i64 4
  %p5 = getelementptr inbounds b8, b8* %p0, i64 5
  %p6 = getelementptr inbounds b8, b8* %p0, i64 6
  %p7 = getelementptr inbounds b8, b8* %p0, i64 7
  %b0 = load b8, b8* %p0, align 1
  %b1 = load b8, b8* %p1, align 1
  %b2 = load b8, b8* %p2, align 1
  %b3 = load b8, b8* %p3, align 1
  %b4 = load b8, b8* %p4, align 1
  %b5 = load b8, b8* %p5, align 1
  %b6 = load b8, b8* %p6, align 1
  %b7 = load b8, b8* %p7, align 1
  %x0 = bytecast b8 %b0 to i8
  %x1 = bytecast b8 %b1 to i8
  %x2 = bytecast b8 %b2 to i8
  %x3 = bytecast b8 %b3 to i8
  %x4 = bytecast b8 %b4 to i8
  %x5 = bytecast b8 %b5 to i8
  %x6 = bytecast b8 %b6 to i8
  %x7 = bytecast b8 %b7 to i8
  %v0 = insertelement <8 x i8> poison, i8 %x0, i32 0
  %v1 = insertelement <8 x i8>   %v0, i8 %x1, i32 1
  %v2 = insertelement <8 x i8>   %v1, i8 %x2, i32 2
  %v3 = insertelement <8 x i8>   %v2, i8 %x3, i32 3
  %v4 = insertelement <8 x i8>   %v3, i8 %x4, i32 4
  %v5 = insertelement <8 x i8>   %v4, i8 %x5, i32 5
  %v6 = insertelement <8 x i8>   %v5, i8 %x6, i32 6
  %v7 = insertelement <8 x i8>   %v6, i8 %x7, i32 7
  ret <8 x i8> %v7
}
